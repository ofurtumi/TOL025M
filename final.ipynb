{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "Upphaflega f√≥r skr√∂pun fram √≠ √æessu skjali en √°kva√∞ a√∞ splitta √æv√≠ upp svo h√¶gt v√¶ri a√∞ keyra alla gpt-√æj√°lfunina √° collab √°n √æess a√∞ √æurfa keyra allt drasli√∞ aftur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install tokenizers\n",
    "%pip install transformers\n",
    "%pip install seqeval\n",
    "%pip install accelerate==0.24.1\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> H√©r g√∂gnum vi√∞ √∫r skugga um √æa√∞ a√∞ vi√∞ s√©um me√∞ alla √æ√° pakka sem √æarf til a√∞ keyra notebooki√∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 √æj√°lfun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form√∂ttum g√∂gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 3332\n",
      "Test dataset length: 588\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('scraping/scraped_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "lyrics = list(data.values())\n",
    "for i in range(len(lyrics)):\n",
    "    lyrics[i] = \"\\n\".join(lyrics[i])\n",
    "\n",
    "def build_text_files(data_json, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for texts in data_json:\n",
    "        data += texts + \"  \"\n",
    "    f.write(data)\n",
    "\n",
    "train, test = train_test_split(lyrics,test_size=0.15)\n",
    "\n",
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')\n",
    "\n",
    "print(\"Train dataset length: \"+str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Skiptum g√∂gnunum upp √≠ train- og testdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerum g√∂gnin \"√æj√°lfanleg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (118490 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jonfd/gpt2-igc-is\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stillum √æj√°lfarann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"jonfd/gpt2-igc-is\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-textar\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=16, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=128,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=600,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √ûj√°lfum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vistum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "gpt2-textar is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/gpt2-textar/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1346\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1345\u001b[0m     \u001b[39m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1347\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1233\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1234\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1235\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1236\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1237\u001b[0m     )\n\u001b[1;32m   1238\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1239\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1608\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1599\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1600\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1601\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[0;32m-> 1608\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1610\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6567ba38-73fbd4410aa2b44304e690e3;967e2618-2548-47bc-8a04-286add794e2c)\n\nRepository Not Found for url: https://huggingface.co/gpt2-textar/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/tumi/school/languages/Lokaverkefni/final.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tumi/school/languages/Lokaverkefni/final.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModel\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tumi/school/languages/Lokaverkefni/final.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgpt2-textar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tumi/school/languages/Lokaverkefni/final.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39mpush_to_hub(\u001b[39m\"\u001b[39m\u001b[39mofurtumi/gpt2textar_v0.1\u001b[39m\u001b[39m\"\u001b[39m, private\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:487\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m commit_hash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    486\u001b[0m         \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m         resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    488\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    489\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    490\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    491\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    493\u001b[0m         )\n\u001b[1;32m    494\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    495\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:450\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    457\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: gpt2-textar is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"gpt2-textar\")\n",
    "trainer.push_to_hub(\"ofurtumi/gpt2textar_v0.1\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen1 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.1\", tokenizer=\"jonfd/gpt2-igc-is\", pad_token_id=50256)\n",
    "gen2 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.2\", tokenizer=\"jonfd/gpt2-igc-is\", pad_token_id=50256)\n",
    "gen3 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.3\", tokenizer=\"jonfd/gpt2-igc-is\", pad_token_id=50256)\n",
    "\n",
    "versions = {\"v0.1\": gen1, \"v0.2\": gen2, \"v0.3\": gen3}\n",
    "# gen03 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.1\", tokenizer=\"jonfd/gpt2-igc-is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_versions(prompt):\n",
    "    for name, version in versions.items():\n",
    "        print(name)\n",
    "        print(version(prompt, do_sample=True, temperature=0.9)[0][\"generated_text\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "Kisuv√¶l\n",
      "Komdu og d√∂nsu√∞u fyrir mig\n",
      "Komdu og d√∂nsu√∞u fyrir mig\n",
      "Komdu og d√∂nsu√∞u fyrir mig\n",
      "Komdu og dansa√∞u fyrir mig\n",
      "Komdu og dansa√∞u fyrir mig\n",
      "Kom\n",
      "\n",
      "v0.2\n",
      "Kisuv√¶l.\n",
      "L√≥an √° morgnanna fer √° stj√°\n",
      "og l√≥an √° h√°degi syngur sitt lag\n",
      "L√≥an √° morgnana fer √° stj√°\n",
      "og l√≥an √° h√°degi syngur sitt lag\n",
      "Og √æegar klukk\n",
      "\n",
      "v0.3\n",
      "Kisuv√¶l\"√âg get ekki sungi√∞,\n",
      "√©g get ekki sungi√∞ me√∞ √æ√©r\n",
      "√ûa√∞ geta allir sungi√∞\n",
      "√ûa√∞ geta allir sungi√∞\n",
      "√æa√∞ geta allir gr√°ti√∞.  √âg hitti hana √° bar √≠ Berl√≠n.\n",
      "H√∫n brosti bl√≠tt til m√≠n\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"Kisuv√¶l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "F√≥lk sem hefur alltof gaman af √æv√≠ a√∞ hafa gaman,\n",
      "svona venjulegt f√≥lk,\n",
      "√©g myndi segja a√∞ √æa√∞ v√¶ri komi√∞ n√≥g.\n",
      "En kannski var √æetta allt saman vitleysa.  √âg er ekki einn af √æeim sem nenna a√∞ hanga √° kl√≥settinu\n",
      "\n",
      "v0.2\n",
      "F√≥lk sem hefur alltof gaman\n",
      "er l√∂ngu h√¶tt a√∞ s√Ωna √° s√©r sparihli√∞arnar.\n",
      "√âg er viss um a√∞ √©g geti sagt ykkur,\n",
      "√©g er nokku√∞ viss um a√∞ √©g geti sagt ykkur,\n",
      "√©g er nokku√∞ viss um a√∞ √©g geti sagt\n",
      "\n",
      "v0.3\n",
      "F√≥lk sem hefur alltof gaman af √æv√≠ sem √æa√∞ s√©r\n",
      "√Å m√≥ti kemur samt a√∞ √æa√∞ er ekki √æannig\n",
      "Freistingarnar eru til a√∞ vaxa og vaxa en ekkert nema vonleysi\n",
      "√ûa√∞ er ekki t√∂ff a√∞ vera grannur\n",
      "√ûa√∞ er ekki t√∂ff a√∞\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"F√≥lk sem hefur alltof gaman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "Texti um barn sem t√Ωndi boltanum s√≠num\n",
      "og t√≥k hann til s√≠n\n",
      "√©g fann √æa√∞ strax √≠ fyrstu hendi\n",
      "√ûa√∞ var l√≠ti√∞ um svefn √≠ √æessum b√¶\n",
      "og f√°tt sem var √° sveimi\n",
      "vi√∞ f√≥rum √≠ labbit√∫r me√∞ honum\n",
      "√©g\n",
      "\n",
      "v0.2\n",
      "Texti um barn sem t√Ωndi boltanum s√≠num\n",
      "En n√∫ er hann kominn √° lei√∞arenda\n",
      "Og pabbi hans, er kominn heim\n",
      "Og pabbi hans, er kominn heim\n",
      "Er kominn heim  Skotta litla fer √≠ bankann\n",
      "og tekur √∫t\n",
      "Kass\n",
      "\n",
      "v0.3\n",
      "Texti um barn sem t√Ωndi boltanum s√≠num\n",
      "Bamm bamm bamm bamm bamm belj\n",
      "Bamm bamm bamm bamm\n",
      "Bamm bamm bamm bamm belj\n",
      "Bamm bamm bamm bamm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"Texti um barn sem t√Ωndi boltanum s√≠num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "J√≥lah√∫fan er flott og hl√Ω\n",
      "h√∫n er g√≥√∞hestur og gaman a√∞ fara √≠ hana.\n",
      "N√∫ er h√∫n farin a√∞ draga mig\n",
      "hittist vi√∞ hana √∂mmu,\n",
      "h√∫n sag√∞i nei, nei, vi√∞ mig.\n",
      "\n",
      "\n",
      "v0.2\n",
      "J√≥lah√∫fan er flott\n",
      "Vi√∞ skulum fara √≠ lautarfer√∞\n",
      "√ûv√≠ a√∞ √æetta er h√∫n.\n",
      "Vi√∞ notum hana √≠ lautarfer√∞\n",
      "Og s√≠√∞an f√∂rum vi√∞ √≠ lautarfer√∞  √Å dimmri n√≥tt √©g leggst upp √≠ r√∫m til √æ√≠n\n",
      "√âg veit\n",
      "\n",
      "v0.3\n",
      "J√≥lah√∫fan er flott\n",
      "√∫r √æv√≠ sem komi√∞ er\n",
      "og √∫r √æv√≠ sem komi√∞ er\n",
      "og af √æv√≠ √©g veit\n",
      "A√∞ h√©r var √æa√∞ fyrir einum 15 √°rum\n",
      "a√∞ b√∫a √≠ tjaldi\n",
      "Og eiga me√∞ okkur kofa\n",
      "√≠ stormi\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"J√≥lah√∫fan er flott\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "Bj√≥r er g√≥√∞ur.\n",
      "Bj√≥r er g√≥√∞ur.\n",
      "Bj√≥r er g√≥√∞ur.\n",
      "Bj√≥r er g√≥√∞ur.\n",
      "Vi√∞ erum √° √æingi\n",
      "og √∂ll okkar vandam√°l eru leyst.\n",
      "Og vi√∞ l√°tum ekki neitt st√∂√∞va okkur.\n",
      "Bj√≥r er g√≥√∞ur\n",
      "\n",
      "v0.2\n",
      "Bj√≥r er g√≥√∞ur\n",
      "m√©r finnst gott  Lestin er √° enda,\n",
      "sleppa gatan svo √ær√∂ng.\n",
      "Hopp og h√¶ og hall√≥.\n",
      "√âg gef henni sans,\n",
      "alloft kenndur finnings\n",
      "√∫t √° l√≠fi√∞\n",
      "\n",
      "v0.3\n",
      "Bj√≥r er g√≥√∞ur\n",
      "√æ√∫ veist a√∞ hann er g√≥√∞ur\n",
      "√æ√∫ veist a√∞ hann er g√≥√∞ur\n",
      "√æ√∫ veist a√∞ hann er svalur\n",
      "√æ√∫ veist a√∞ hann er feitur\n",
      "√æ√∫ veist a√∞ hann er √¶√∞i\n",
      "√æ√∫ veist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"Bj√≥r er g√≥√∞ur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
